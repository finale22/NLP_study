{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn6P-_XmOVBX"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples = [\"AlphaGo has intelligized through deep learning.\", \"Recurrent neural networks are a kind of deep learning algorithm.\"]"
      ],
      "metadata": {
        "id": "OO5i3M_lOgB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dimensionality = 1000 # 크기가 1000인 벡터로 단어를 저장. 1000개 이상의 단어가 있다면 해싱 충돌이 늘어나고 인코딩의 정확도가 감소\n",
        "max_len = 10 # 샘플에서 max_len까지의 단어만 사용\n",
        "\n",
        "results = np.zeros((len(samples), max_len, dimensionality)) # 해싱 결과를 저장할 넘파이 배열"
      ],
      "metadata": {
        "id": "6HfnH0gaOy3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[:max_len]:\n",
        "        print(\"word: \", word)\n",
        "        hash_word = hash(word)\n",
        "        print(\"hash(word): \", hash_word)\n",
        "        abs_hash_word = abs(hash_word)\n",
        "        print(\"abs(hash(word)): \", abs_hash_word)\n",
        "        index = abs_hash_word % dimensionality\n",
        "        print(\"index: \", index)\n",
        "        results[i, j, index] = 1.\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gnz-419gQrGo",
        "outputId": "04d858a6-0f88-4df6-d138-8ae2197f7ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word:  AlphaGo\n",
            "hash(word):  -6478401417523475540\n",
            "abs(hash(word)):  6478401417523475540\n",
            "index:  540\n",
            "word:  has\n",
            "hash(word):  3003061812357373790\n",
            "abs(hash(word)):  3003061812357373790\n",
            "index:  790\n",
            "word:  intelligized\n",
            "hash(word):  6108892223904572912\n",
            "abs(hash(word)):  6108892223904572912\n",
            "index:  912\n",
            "word:  through\n",
            "hash(word):  -6005411517147695028\n",
            "abs(hash(word)):  6005411517147695028\n",
            "index:  28\n",
            "word:  deep\n",
            "hash(word):  -6275315076995335766\n",
            "abs(hash(word)):  6275315076995335766\n",
            "index:  766\n",
            "word:  learning.\n",
            "hash(word):  -5153763055654257247\n",
            "abs(hash(word)):  5153763055654257247\n",
            "index:  247\n",
            "word:  Recurrent\n",
            "hash(word):  -3137305838437918874\n",
            "abs(hash(word)):  3137305838437918874\n",
            "index:  874\n",
            "word:  neural\n",
            "hash(word):  -2291973141328787303\n",
            "abs(hash(word)):  2291973141328787303\n",
            "index:  303\n",
            "word:  networks\n",
            "hash(word):  792144246047159534\n",
            "abs(hash(word)):  792144246047159534\n",
            "index:  534\n",
            "word:  are\n",
            "hash(word):  4811852616200324968\n",
            "abs(hash(word)):  4811852616200324968\n",
            "index:  968\n",
            "word:  a\n",
            "hash(word):  -1381678511694179621\n",
            "abs(hash(word)):  1381678511694179621\n",
            "index:  621\n",
            "word:  kind\n",
            "hash(word):  -6611088079687422480\n",
            "abs(hash(word)):  6611088079687422480\n",
            "index:  480\n",
            "word:  of\n",
            "hash(word):  -8395606047607423480\n",
            "abs(hash(word)):  8395606047607423480\n",
            "index:  480\n",
            "word:  deep\n",
            "hash(word):  -6275315076995335766\n",
            "abs(hash(word)):  6275315076995335766\n",
            "index:  766\n",
            "word:  learning\n",
            "hash(word):  -7535715173001044899\n",
            "abs(hash(word)):  7535715173001044899\n",
            "index:  899\n",
            "word:  algorithm.\n",
            "hash(word):  6979526981548750387\n",
            "abs(hash(word)):  6979526981548750387\n",
            "index:  387\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n"
          ]
        }
      ]
    }
  ]
}